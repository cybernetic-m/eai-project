{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3O4mXmBol0t"
      },
      "source": [
        "# IMPORT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab # type: ignore\n",
        "    colab = True\n",
        "except:\n",
        "    colab = False\n",
        "\n",
        "if colab:\n",
        "    !git clone \"https://github.com/cybernetic-m/eai-project.git\" # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gQw18CXol0v"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import ExponentialLR \n",
        "\n",
        "# Others\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import pickle\n",
        " \n",
        "# Our files\n",
        "if colab:\n",
        "    sys.path.append('/content/eai-project/training')\n",
        "    sys.path.append('/content/eai-project/preprocessing')\n",
        "    sys.path.append('/content/eai-project/dataset')\n",
        "    sys.path.append('/content/eai-project/utils')\n",
        "    sys.path.append('/content/eai-project/models')\n",
        "    sys.path.append('/content/eai-project/modules')\n",
        "    sys.path.append('/content/eai-project/testing')\n",
        "    sys.path.append('/content/eai-project')\n",
        "    from train import train\n",
        "    from preprocessing import *\n",
        "    from thermal_dataset import thermal_dataset \n",
        "    from csv_utils import *\n",
        "    from complete_model import complete_model \n",
        "    from testing.test import test\n",
        "    from blocks import mlp, linear, rnn, lstm\n",
        "    prefix = '/content'\n",
        "        \n",
        "else:\n",
        "    from training.train import train\n",
        "    from preprocessing.preprocessing import *\n",
        "    from dataset.thermal_dataset import thermal_dataset\n",
        "    from utils.csv_utils import *\n",
        "    from models.complete_model import complete_model\n",
        "    from testing.test import test\n",
        "    from blocks import mlp, linear, rnn, lstm\n",
        "    prefix = '.'\n",
        "    \n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyper_parameters = {'num_epochs': 100,\n",
        "                    'batch_size': 256,\n",
        "                    'lr': 0.0004,\n",
        "                    'mode': 'auto-weighted', #average, median, auto-weighted\n",
        "                    'extractor_type': 'lstm_autoencoder', #lstm_autoencoder (if you want to use the lstm autoencoder), conv (if you want to use the convolutional autoencoder) or lstm_encoder (if you want to use only the lstm encoder)\n",
        "                    'timesteps': 200,\n",
        "                    'window_size': 30,\n",
        "                    'norm': 'Std', # Not (Raw Data), Minmax (MinMax Scaling), Std (Standard Scaling)\n",
        "                    'file':1,\n",
        "                    'pretrain':False,\n",
        "                    'autoencoder_train' : False, # to train only the autoencoder\n",
        "                    'heterogeneous': True,\n",
        "                    'lr_multipliers_extractor': 20,\n",
        "                    'lr_multipliers_ensemble': {\n",
        "                        mlp: 1,\n",
        "                        linear: 1,\n",
        "                        rnn: 50,\n",
        "                        lstm: 50,\n",
        "                    },\n",
        "                    'gamma': 0.85\n",
        "                        }\n",
        "\n",
        "# Parameters of the convolutional autoencoder\n",
        "conv_autoencoder_dict = {'in_kern_out': [[4, 5, 5],[5, 5, 6]], # List of hyperparam of autoencoder [[in_channels, kernel_size, out_channels], ...]\n",
        "                    'pooling_kernel_size': 2, # how much big is the kernel of the pooling (i.e. 2 means halving the dimension each layer)\n",
        "                    'padding': 'same', # 'same', 'full', 'valid'\n",
        "                    'pooling': 'avg', # 'max' for Max Pooling and 'avg' for Average Pooling \n",
        "                    'scale_factor': 2, # upsample scale_factor, 2 means double the dimension each layer\n",
        "                    'upsample_mode': 'linear', # mode of the upsampling 'linear', 'nearest'\n",
        "                    'dropout': 0.5\n",
        " }\n",
        "\n",
        "lstm_autoencoder_dict = {\"in_hidd\": [[4, 2]],\n",
        "                         \"dropout\": 0.15,\n",
        "                         \"lstm_layers\": 2}\n",
        "\n",
        "\n",
        "if hyper_parameters['extractor_type'] == 'conv':\n",
        "    feature_dim = int(( hyper_parameters['timesteps'] / (2**(len(conv_autoencoder_dict['in_kern_out'])-1))*conv_autoencoder_dict['in_kern_out'][-1][-1]) /2)\n",
        "    autoencoder_dict = conv_autoencoder_dict\n",
        "elif 'lstm' in hyper_parameters['extractor_type']:\n",
        "    feature_dim = hyper_parameters['timesteps']*lstm_autoencoder_dict['in_hidd'][-1][-1] \n",
        "    autoencoder_dict = lstm_autoencoder_dict\n",
        "if hyper_parameters['heterogeneous']:\n",
        "    feature_dim += 3\n",
        "    \n",
        "\n",
        "# Definition of the model (You can decomment to include more models)\n",
        "if hyper_parameters['autoencoder_train'] == False:\n",
        "    ensemble_model = {\n",
        "        'mlp': [{'layer_dim_list': [ feature_dim,int(feature_dim*1.5),int(feature_dim/2),int(feature_dim/4),int(feature_dim/6),int(feature_dim/8),int(feature_dim/10),int(feature_dim/12),int(feature_dim/14),int(feature_dim/16),int(feature_dim/18),3], 'bias':True}],\n",
        "        #        {'layer_dim_list': [ feature_dim,int(feature_dim*1.5),int(feature_dim/1.5),3]}], \n",
        "        'ARIMA': [{'p': 2, 'd': 0, 'q': 2, 'ps': 0, 'ds': 0, 'qs': 0, 's': 1}], \n",
        "        #'linear_regressor': [{'in_features': feature_dim, 'out_features': 3, 'bias':False}],\n",
        "        'lstm': [{'feature_dim':int(feature_dim/4), 'input_dim':feature_dim, 'out_features': 3, 'bias':False, 'num_layers':2},\n",
        "                {'feature_dim':int(feature_dim/5), 'input_dim':feature_dim, 'out_features': 3, 'bias':False, 'num_layers':4}\n",
        "                ],\n",
        "        'rnn': [{'feature_dim':int(feature_dim/2), 'input_dim':feature_dim, 'out_features': 3, 'bias':False, 'num_layers':16}],\n",
        "    }\n",
        "\n",
        "else:\n",
        "    ensemble_model = {} # Set the dictionary of the ensemble model to void if we want to train only the autoencoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data already exists!\n",
            "./data/Xtraining1.npy\n",
            "./data/Y30training1.npy\n"
          ]
        }
      ],
      "source": [
        "skip = False\n",
        "\n",
        "if os.path.exists(prefix+'/data/X'+'training'+str(hyper_parameters['file'])+'.npy') and os.path.exists(prefix+'/data/Y'+str(hyper_parameters['window_size'])+'training'+str(hyper_parameters['file'])+'.npy'):\n",
        "    print(\"Data already exists!\")\n",
        "    print(prefix+'/data/X'+'training'+str(hyper_parameters['file'])+'.npy')\n",
        "    print(prefix+'/data/Y'+str(hyper_parameters['window_size'])+'training'+str(hyper_parameters['file'])+'.npy')\n",
        "    skip = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reproducibility and Device Setting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set a seed for reproducibility purposes\n",
        "seed = 46\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Set the device (cuda for Nvidia GPUs, mps for M1, M2 .. Apple Silicon)\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFQrCrM-ol0w"
      },
      "source": [
        "# DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mHN7T9Rol0x",
        "outputId": "6d8b8eef-3135-4b49-f113-9ac4548ea7aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file already downloaded!\n",
            "CSV file already unzipped!\n"
          ]
        }
      ],
      "source": [
        "link_zipped_csv = 'https://drive.google.com/file/d/1MssQF4pI_rZqiiDBP4XaLTT1ZaN6ykLm/view?usp=drive_link'\n",
        "gdrive_link = 'https://drive.google.com/uc?id='\n",
        "csv_dir = './csv'\n",
        "zipped_file = './csv.zip'\n",
        "\n",
        "download_csv(\n",
        "    link_zipped_csv,\n",
        "    gdrive_link,\n",
        "    zipped_file\n",
        ")\n",
        "\n",
        "unzip_csv(\n",
        "    zipped_file,\n",
        "    csv_dir,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eKLx-ldvGxvs"
      },
      "outputs": [],
      "source": [
        "if not skip:\n",
        "\n",
        "    path = '/content/csv/thermal_drift_features_lab_05_02.csv'\n",
        "\n",
        "    # Read all the CSV files containing the Temperatures\n",
        "    features_1 = pd.read_csv(os.path.join(prefix,'csv/thermal_drift_features_lab_05_02.csv'))\n",
        "    features_2 = pd.read_csv(os.path.join(prefix, 'csv/thermal_drift_features_lab_05_03.csv'))\n",
        "    features_3 = pd.read_csv(os.path.join(prefix,'csv/thermal_drift_features_lab_05_04.csv'))\n",
        "    features_4 = pd.read_csv(os.path.join(prefix,'csv/thermal_drift_features_lab_05_05.csv'))\n",
        "    features_5 = pd.read_csv(os.path.join(prefix,'csv/thermal_drift_features_lab_05_06.csv'))\n",
        "\n",
        "    # Read all the CSV files containing the X1, Y1, Z1 \n",
        "    targets_1 = pd.read_csv(os.path.join(prefix,'csv/thermal_drift_targets_lab_05_02.csv'))\n",
        "    targets_2 = pd.read_csv(os.path.join(prefix,'csv/thermal_drift_targets_lab_05_03.csv'))\n",
        "    targets_3 = pd.read_csv(os.path.join(prefix,'csv/thermal_drift_targets_lab_05_04.csv'))\n",
        "    targets_4 = pd.read_csv(os.path.join(prefix,'csv/thermal_drift_targets_lab_05_05.csv'))\n",
        "    targets_5 = pd.read_csv(os.path.join(prefix,'csv/thermal_drift_targets_lab_05_06.csv'))\n",
        "\n",
        "    features = [features_1, features_2, features_3, features_4, features_5]\n",
        "    targets = [targets_1,targets_2,targets_3,targets_4,targets_5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not skip:\n",
        "    for feature, target in zip(features, targets):\n",
        "        feature.drop([\n",
        "            \"name\", \"tags\",\n",
        "            \"2\\\"Tray1 Vacuum Sensor\", \"2\\\"Tray2 Vacuum Sensor\", \"2\\\"Tray3 Vacuum Sensor\",\n",
        "            \"Avg Oven Temperature\", \"Chuck Temp [Cdeg]\", \"Chuck Temp2 [Cdeg]\",\n",
        "            \"Chuck1 Vacuum Sensor\", \"Contrast\", \"Device State\",\n",
        "            \"Dispenser1 Pressure Sensor\", \"Machine Room Temp\", \"Main Air\", \"Main Vacuum\",\n",
        "            \"Oven Temperature\", \"PE_Rx\", \"PE_Ry\", \"PE_Rz\", \"PE_X1\", \"PE_Y1\", \"PE_Z1\",\n",
        "            \"PUT1 Flow Sensor\", \"PUT2 Flow Sensor1\", \"PUT2 Flow Sensor2\",\n",
        "            \"PUT2 Flow Sensor3\", \"PUT2 Flow Sensor4\", \"PUT2 Flow Sensor5\",\n",
        "            \"Photodiode\", \"Pixel Power\", \"Preciser1 Vacuum Sensor\",\n",
        "            \"Tec FIB1 Holder\", \"Tec FIB1 Plate\", \"Tec FIB2 Holder\", \"Tec FIB2 Plate\",\n",
        "            \"Torque11\",\"Torque2\",\"Torque3\",\"Torque4\",\"Torque5\",\"Torque6\"\n",
        "        ], axis=1, inplace=True)\n",
        "        if 'name' in target.keys() and 'tags' in target.keys():\n",
        "\n",
        "            target.drop(['name', 'tags'], axis=1, inplace=True)\n",
        "            \n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "qUZH1JpDfNY0",
        "outputId": "694acf4b-ac9a-401f-f4ef-d6b8105ebf1b"
      },
      "outputs": [],
      "source": [
        "if not skip:\n",
        "    print(features[1]) # Print the features_1 table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "OtsO4hF1fHUU",
        "outputId": "3b3e33b6-a0eb-474b-814a-67338bee9fdd"
      },
      "outputs": [],
      "source": [
        "if not skip:\n",
        "\n",
        "    print(targets[1]) # Print the target_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "H7L8dLyMmesB"
      },
      "outputs": [],
      "source": [
        "if not skip:\n",
        "    # Put X1, Y1, Z1 on the same row of X1 eliminating the NAN values\n",
        "    fixed_targets = [] # Create a list of target in which we put X1, Y1, Z1 in the same row\n",
        "    for target in targets:\n",
        "        fixed_targets.append(transform_dataframe(target)) # iterate over target_1,2,3 ... and append in fixed_targets\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "23LmYWUfmqDM",
        "outputId": "fc9873f7-b53c-4dbc-a2bd-b0cf9fab7f2c"
      },
      "outputs": [],
      "source": [
        "if not skip:\n",
        "\n",
        "    print(fixed_targets[1]) # Print the fixed_target_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DOVEQQeQr36x"
      },
      "outputs": [],
      "source": [
        "if not skip:\n",
        "    # Merge of targets with features in one single dataframe\n",
        "    complete_numbers_list = [] # List of the table with columns that are numbers (0,1,2..) in which we unify both features and targets merging on closest time row\n",
        "    for fixed_target, feature in zip(fixed_targets, features):\n",
        "        complete_numbers_list.append(merge_on_closest_time(fixed_target.reset_index(), feature.reset_index()))\n",
        "\n",
        "    trainig_number_list = []\n",
        "    testing_number_list = []\n",
        "    for i in range(len(complete_numbers_list)):\n",
        "        part_numbers_list = complete_numbers_list[:i] + complete_numbers_list[i+1:]\n",
        "        trainig_number_list.append(pd.concat(part_numbers_list))\n",
        "        testing_number_list.append(complete_numbers_list[i])\n",
        "        \n",
        "    complete_numbers_dataframe = pd.concat(complete_numbers_list)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not skip: \n",
        "    print(complete_numbers_list) # Print of one example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "m39ldRHjsbJH"
      },
      "outputs": [],
      "source": [
        "if not skip:\n",
        "\n",
        "    complete = complete_numbers_dataframe.rename(columns={\n",
        "        0: 'id',\n",
        "        1: 'time',\n",
        "        2: 'X1',\n",
        "        3: 'Y1',\n",
        "        4: 'Z1',\n",
        "        5: 'to_remove',\n",
        "        6: 'time_2',\n",
        "        7: 'Temp1',\n",
        "        8: 'Temp2',\n",
        "        9: 'Temp3',\n",
        "        10: 'Temp4'\n",
        "        })\n",
        "    complete.drop(['time', 'to_remove', 'time_2'], axis=1, inplace=True)\n",
        "    training_list = []\n",
        "    testing_list = []\n",
        "    for training, testing in zip(trainig_number_list, testing_number_list):\n",
        "        training_tmp = training.rename(columns={\n",
        "            0: 'id',\n",
        "            1: 'time',\n",
        "            2: 'X1',\n",
        "            3: 'Y1',\n",
        "            4: 'Z1',\n",
        "            5: 'to_remove',\n",
        "            6: 'time_2',\n",
        "            7: 'Temp1',\n",
        "            8: 'Temp2',\n",
        "            9: 'Temp3',\n",
        "            10: 'Temp4'\n",
        "            })\n",
        "        training_tmp.drop(['time', 'to_remove', 'time_2'], axis=1, inplace=True)\n",
        "        training_list.append(training_tmp)\n",
        "        \n",
        "        testing_tmp = testing.rename(columns={\n",
        "            0: 'id',\n",
        "            1: 'time',\n",
        "            2: 'X1',\n",
        "            3: 'Y1',\n",
        "            4: 'Z1',\n",
        "            5: 'to_remove',\n",
        "            6: 'time_2',\n",
        "            7: 'Temp1',\n",
        "            8: 'Temp2',\n",
        "            9: 'Temp3',\n",
        "            10: 'Temp4'\n",
        "            })\n",
        "        testing_tmp.drop(['time', 'to_remove', 'time_2'], axis=1, inplace=True)\n",
        "        testing_list.append(testing_tmp)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not skip:\n",
        "\n",
        "    print(training_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if not skip:\n",
        "    \n",
        "    # Save the features and targets in file npy\n",
        "    for i, (testing, training) in enumerate(zip(testing_list, training_list)):\n",
        "\n",
        "        # Transform the training and test data in float\n",
        "        training.astype(float) \n",
        "        testing.astype(float) \n",
        "\n",
        "        # Take from dataframe the values of the columns of temperatures and positions saving into smallest dataframe of training/test\n",
        "        X_train = training[['Temp1','Temp2', 'Temp3', 'Temp4']] \n",
        "        Y_train = training[['X1', 'Y1', 'Z1']] \n",
        "        X_test = testing[['Temp1','Temp2', 'Temp3', 'Temp4']] \n",
        "        Y_test = testing[['X1', 'Y1', 'Z1']] \n",
        "\n",
        "        # Transform the X, Y from dataframe in numpy array both for test and train\n",
        "        X_train = X_train.values.astype(np.float32) \n",
        "        Y_train = Y_train.values.astype(np.float32) \n",
        "        X_test = X_test.values.astype(np.float32) \n",
        "        Y_test = Y_test.values.astype(np.float32) \n",
        "\n",
        "        # Do the gradient of the positions both for test and train\n",
        "        Y_train = my_gradient(Y_train, window_size=hyper_parameters['window_size']) \n",
        "        Y_test = my_gradient(Y_test, window_size=hyper_parameters['window_size']) \n",
        " \n",
        "        np.save(prefix+'/data'+'/X'+'training'+str(i)+'.npy',X_train)  \n",
        "        np.save(prefix+'/data'+'/Y'+str(hyper_parameters['window_size'])+'training'+str(i)+'.npy',Y_train)\n",
        "        np.save(prefix+'/data'+'/X'+'testing'+str(i)+'.npy',X_test)\n",
        "        np.save(prefix+'/data'+'/Y'+str(hyper_parameters['window_size'])+'testing'+str(i)+'.npy',Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not skip:\n",
        "\n",
        "    targets_1['time'] = pd.to_datetime(targets_1['time'], unit='ns')\n",
        "\n",
        "    # Reshape the DataFrame using melt()\n",
        "    targets_melted = targets_1.reset_index().melt(id_vars=['time'], value_vars=['X1', 'Y1', 'Z1'], var_name='variable', value_name='value')\n",
        "\n",
        "    # Drop rows where 'value' is NaN (to keep only the non-null entries)\n",
        "    targets_melted = targets_melted.dropna(subset=['value'])\n",
        "\n",
        "    # Plot the data\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for var in targets_melted['variable'].unique():\n",
        "        # Filter data for each variable and plot\n",
        "        temp_df = targets_melted[targets_melted['variable'] == var]\n",
        "        plt.plot(temp_df['time'], temp_df['value'], label=var)\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Values')\n",
        "    plt.title('Plot of X1, Y1, Z1 over Time')\n",
        "    plt.legend(title='Variable')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "    #targets.plot(y='X1',x='time')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not skip:\n",
        "\n",
        "    # Convert 'time' to datetime (nanoseconds to datetime)\n",
        "    features_1['time'] = pd.to_datetime(features_1['time'], unit='ns')\n",
        "\n",
        "    # Set 'time' as the index\n",
        "    features_1.set_index('time', inplace=True)\n",
        "\n",
        "    # Optionally, you can plot X1, Y1, Z1 directly\n",
        "    features_1[['Temp Sensor 1', 'Temp Sensor 2', 'Temp Sensor 3', 'Temp Sensor 4', ]].plot()\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Values')\n",
        "    plt.title('Plot of Temp Sensor 1, Temp Sensor 2, Temp Sensor 3, Temp Sensor 4 over Time')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Value [temp1, temp2, temp3, temp4]: 2117.257568359375 2155.469970703125 2150.99951171875 2157.142578125 \n",
            "Standard Deviation [temp1, temp2, temp3, temp4]: 22.64933204650879 23.240192413330078 21.471412658691406 20.387102127075195\n",
            "Mean Value [X1, Y1, Z1]: 5.152535595698282e-05 3.1649022275814787e-05 -1.0432697905571331e-07 \n",
            "Standard Deviation  [X1, Y1, Z1]: 0.0024545718915760517 0.009821190498769283 0.009821190498769283\n"
          ]
        }
      ],
      "source": [
        "X = np.load(prefix+'/data'+'/X'+'training'+str(hyper_parameters['file'])+'.npy') # Training data\n",
        "Y = np.load(prefix+'/data'+'/Y'+str(hyper_parameters['window_size'])+'training'+str(hyper_parameters['file'])+'.npy')\n",
        "\n",
        "mean_X, std_X, min_val_X, max_val_X = torch.tensor([1.0, 1.0, 1.0, 1.0]), torch.tensor([0.0, 0.0, 0.0, 0.0]), torch.tensor([-1.0, -1.0, -1.0, -1.0]), torch.tensor([1.0,1.0, 1.0, 1.0]) # Initialization of values\n",
        "mean_Y, std_Y, min_val_Y, max_val_Y = torch.tensor([1.0,1.0,1.0]), torch.tensor([0.0,0.0,0.0]), torch.tensor([-1.0,-1.0,-1.0]), torch.tensor([1.0,1.0,1.0]) # Initialization of values\n",
        "\n",
        "#hyper_parameters['norm'] = 'Minmax'\n",
        "# Fit the scaling to understand the parameters of the training set\n",
        "if hyper_parameters['norm'] != 'Not':\n",
        "    if hyper_parameters['norm'] == 'Minmax':\n",
        "        scaler_X = MinMaxScaler()\n",
        "        scaler_Y = MinMaxScaler()\n",
        "        scaler_X.fit(X) # Fit only on training data\n",
        "        scaler_Y.fit(Y) # Fit only on training data\n",
        "        min_val_X, max_val_X = torch.tensor(scaler_X.data_min_.astype(np.float32)).to(device), torch.tensor(scaler_X.data_max_.astype(np.float32)).to(device) # Get minimum and maximum values\n",
        "        min_val_Y, max_val_Y = torch.tensor(scaler_Y.data_min_.astype(np.float32)).to(device), torch.tensor(scaler_Y.data_max_.astype(np.float32)).to(device) # Get minimum and maximum values\n",
        "        print(\"Min Value [temp1, temp2, temp3, temp4]:\", min_val_X[0].item(),min_val_X[1].item(), min_val_X[2].item(), min_val_X[3].item(), \"\\nMax Value [temp1, temp2, temp3, temp4]:\", max_val_X[0].item(), max_val_X[1].item(), max_val_X[2].item(), max_val_X[3].item())\n",
        "        print(\"Min Value [X1, Y1, Z1]:\", min_val_Y[0].item(),min_val_Y[1].item(), min_val_Y[2].item(), \"\\nMax Value [X1, Y1, Z1]:\", max_val_Y[0].item(), max_val_Y[1].item(), max_val_Y[2].item())\n",
        "\n",
        "    elif hyper_parameters['norm'] == 'Std':\n",
        "        scaler_X = StandardScaler()\n",
        "        scaler_Y = StandardScaler()\n",
        "        scaler_X.fit(X)  # Fit only on training data\n",
        "        scaler_Y.fit(Y)  # Fit only on training data\n",
        "        mean_X, std_X = torch.tensor(scaler_X.mean_.astype(np.float32)).to(device), torch.tensor(scaler_X.scale_.astype(np.float32)).to(device) # Get mean & std\n",
        "        mean_Y, std_Y = torch.tensor(scaler_Y.mean_.astype(np.float32)).to(device), torch.tensor(scaler_Y.scale_.astype(np.float32)).to(device) # Get mean & std\n",
        "        print(\"Mean Value [temp1, temp2, temp3, temp4]:\", mean_X[0].item(),mean_X[1].item(), mean_X[2].item(), mean_X[3].item(), \"\\nStandard Deviation [temp1, temp2, temp3, temp4]:\", std_X[0].item(), std_X[1].item(), std_X[2].item(), std_X[3].item())\n",
        "        print(\"Mean Value [X1, Y1, Z1]:\", mean_Y[0].item(),mean_Y[1].item(), mean_Y[2].item(), \"\\nStandard Deviation  [X1, Y1, Z1]:\", std_Y[0].item(), std_Y[1].item(), std_Y[1].item())\n",
        "else:\n",
        "    print(\"Using Raw Data!\")\n",
        "\n",
        "#print(X)\n",
        "#print(Y)\n",
        "\n",
        "splitPerc = [0.7,0.3]\n",
        "splitted_X = split(X, splitPerc)\n",
        "splitted_Y = split(Y, splitPerc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnlFgCAkol0x"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasetTrain = thermal_dataset((splitted_X[0],splitted_Y[0]), hyper_parameters['timesteps'], device)\n",
        "datasetVal = thermal_dataset((splitted_X[1],splitted_Y[1]), hyper_parameters['timesteps'], device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbEyWHlGEJHm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autoencoder type: LSTM\n",
            "Autoencoder Summary: lstm_autoencoder(\n",
            "  (encoder): lstm_encoder(\n",
            "    (lstm_layers): ModuleList(\n",
            "      (0): lstm(\n",
            "        (lstm): LSTM(4, 2, num_layers=2, batch_first=True, dropout=0.15)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): lstm_decoder(\n",
            "    (lstm_layers): ModuleList(\n",
            "      (0): lstm(\n",
            "        (lstm): LSTM(2, 4, num_layers=2, batch_first=True, dropout=0.15)\n",
            "      )\n",
            "    )\n",
            "    (linear_decoder): Linear(in_features=800, out_features=800, bias=True)\n",
            "  )\n",
            ")\n",
            "Ensemble Model Summary: ModuleList(\n",
            "  (0): mlp(\n",
            "    (linear_layers): ModuleList(\n",
            "      (0): Linear(in_features=403, out_features=604, bias=True)\n",
            "      (1): Linear(in_features=604, out_features=201, bias=True)\n",
            "      (2): Linear(in_features=201, out_features=100, bias=True)\n",
            "      (3): Linear(in_features=100, out_features=67, bias=True)\n",
            "      (4): Linear(in_features=67, out_features=50, bias=True)\n",
            "      (5): Linear(in_features=50, out_features=40, bias=True)\n",
            "      (6): Linear(in_features=40, out_features=33, bias=True)\n",
            "      (7): Linear(in_features=33, out_features=28, bias=True)\n",
            "      (8): Linear(in_features=28, out_features=25, bias=True)\n",
            "      (9): Linear(in_features=25, out_features=22, bias=True)\n",
            "      (10): Linear(in_features=22, out_features=3, bias=True)\n",
            "    )\n",
            "  )\n",
            ") ModuleList(\n",
            "  (0): ARIMA(\n",
            "    (PD): TwoPolynomialOperation(\n",
            "      (first_poly): BiasOnePolynomial()\n",
            "      (second_poly): Polynomial()\n",
            "    )\n",
            "    (Q): BiasOnePolynomial()\n",
            "    (PDS): TwoPolynomialOperation(\n",
            "      (first_poly): BiasOnePolynomial()\n",
            "      (second_poly): Polynomial()\n",
            "    )\n",
            "    (QS): BiasOnePolynomial()\n",
            "  )\n",
            ") ModuleList(\n",
            "  (0): lstm_regressor(\n",
            "    (lstm): LSTM(403, 100, num_layers=2, batch_first=True)\n",
            "    (linear): Linear(in_features=100, out_features=3, bias=False)\n",
            "  )\n",
            "  (1): lstm_regressor(\n",
            "    (lstm): LSTM(403, 80, num_layers=4, batch_first=True)\n",
            "    (linear): Linear(in_features=80, out_features=3, bias=False)\n",
            "  )\n",
            "  (2): rnn_regressor(\n",
            "    (rnn): RNN(403, 201, num_layers=16, batch_first=True)\n",
            "    (linear): Linear(in_features=201, out_features=3, bias=False)\n",
            "  )\n",
            ")\n",
            "Mode of voting auto-weighted\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Training, Test and Validation Dataloader initialization\n",
        "loaderTrain = DataLoader(datasetTrain, shuffle=True, batch_size=hyper_parameters['batch_size'])\n",
        "loaderVal = DataLoader(datasetVal, shuffle=True, batch_size=hyper_parameters['batch_size'])\n",
        "\n",
        "# Model Initialization (True if you want to use the Ensemble model, False in you want to use a single LSTM model)\n",
        "\n",
        "if hyper_parameters['extractor_type'] == 'conv':\n",
        "    model = complete_model(ensemble_model, \n",
        "                                        device, \n",
        "                                        autoencoder_dim=conv_autoencoder_dict['in_kern_out'], \n",
        "                                        pooling_kernel_size=conv_autoencoder_dict['pooling_kernel_size'], \n",
        "                                        padding=conv_autoencoder_dict['padding'], \n",
        "                                        pooling=conv_autoencoder_dict['pooling'], \n",
        "                                        scale_factor = conv_autoencoder_dict['scale_factor'], \n",
        "                                        upsample_mode=conv_autoencoder_dict['upsample_mode'], \n",
        "                                        dropout=conv_autoencoder_dict['dropout'],\n",
        "                                        mode=hyper_parameters['mode'],\n",
        "                                        heterogeneous=hyper_parameters['heterogeneous'],\n",
        "                                        timesteps=hyper_parameters['timesteps'],\n",
        "                                        norm=hyper_parameters['norm'],\n",
        "                                        mean=(mean_X, mean_Y),\n",
        "                                        std=(std_X, std_Y),\n",
        "                                        min_val= (min_val_X, min_val_Y),\n",
        "                                        max_val= (max_val_X, max_val_Y),\n",
        "                                        ).to(device)\n",
        "\n",
        "elif hyper_parameters['extractor_type'] == 'lstm_autoencoder' or hyper_parameters['extractor_type'] == 'lstm_encoder':\n",
        "    model = complete_model(\n",
        "                                model_dict=ensemble_model, \n",
        "                                device=device, \n",
        "                                timesteps=hyper_parameters['timesteps'],\n",
        "                                lstm_layers=lstm_autoencoder_dict['lstm_layers'],\n",
        "                                autoencoder_dim=lstm_autoencoder_dict['in_hidd'], \n",
        "                                dropout=lstm_autoencoder_dict['dropout'],\n",
        "                                extractor_type=hyper_parameters['extractor_type'],\n",
        "                                heterogeneous=hyper_parameters['heterogeneous'],\n",
        "                                norm=hyper_parameters['norm'],\n",
        "                                mode=hyper_parameters['mode'],\n",
        "                                mean=(mean_X, mean_Y),\n",
        "                                std=(std_X, std_Y),\n",
        "                                min_val= (min_val_X, min_val_Y),\n",
        "                                max_val= (max_val_X, max_val_Y)\n",
        "                                ).to(device)\n",
        "\n",
        "# Definition of the optimizer and loss function\n",
        "models, arima, rnn_ = model.get_models()\n",
        "list_models = models + arima + rnn_ \n",
        "optimizer = [optim.Adam(model.extractor.parameters(), lr=hyper_parameters['lr']*hyper_parameters['lr_multipliers_extractor'])]\n",
        "for sub_model in list_models: \n",
        "    for model_type, multiplier in hyper_parameters['lr_multipliers_ensemble'].items():\n",
        "        if isinstance(sub_model, model_type):\n",
        "            optimizer.append(optim.Adam(model.parameters(), lr=hyper_parameters['lr']*multiplier))\n",
        "            break\n",
        "    \n",
        "hyper_parameters['lr_multipliers_ensemble'] = {\n",
        "        'mlp': hyper_parameters['lr_multipliers_ensemble'][mlp],\n",
        "        'linear': hyper_parameters['lr_multipliers_ensemble'][linear],\n",
        "        'rnn': hyper_parameters['lr_multipliers_ensemble'][rnn],\n",
        "        'lstm': hyper_parameters['lr_multipliers_ensemble'][lstm],\n",
        "    }\n",
        "\n",
        "\n",
        "loss_fn = nn.MSELoss() \n",
        "\n",
        "scheduler = [ExponentialLR(opti, gamma=hyper_parameters['gamma']) for opti in optimizer] \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1/100:\n",
            "Model saved to results/training_2025-03-13_10-33/model.pt\n",
            "Weights of the Ensemble models: [0.1273082047700882, 0.3329713046550751, 0.2220386564731598, 0.18676719069480896, 0.13091465830802917]\n",
            "train MODEL: LOSS 0.000741648124 MAE X1:0.0132, Y1:0.0124, Z1:0.0088 R2 X1:-110.9987, Y1:-7.2525, Z1:-16.0681 RMSE X1:0.027494, Y1:0.033189, Z1:0.019939\n",
            "train AUTOENCODER: LOSS 84.400095047489 MAE temp1:5.8625, temp2:8.7968, temp3:5.2777, temp4:4.5456 R2 temp1:0.8634, temp2:0.7538, temp3:0.8742, temp4:0.8966 RMSE temp1:8.640568, temp2:11.856839, temp3:7.885717, temp4:5.640344\n",
            "valid MODEL: LOSS 0.000003109707 MAE X1:0.0014, Y1:0.0015, Z1:0.0012 R2 X1:0.3406, Y1:0.7093, Z1:0.7502 RMSE X1:0.001669, Y1:0.001932, Z1:0.001677\n",
            "valid AUTOENCODER: LOSS 41.824440149161 MAE temp1:4.8987, temp2:6.0094, temp3:4.4664, temp4:4.5456 R2 temp1:0.9052, temp2:0.8514, temp3:0.9072, temp4:0.8966 RMSE temp1:6.009068, temp2:8.214609, temp3:5.592415, temp4:5.640344\n",
            "EPOCH 2/100:\n",
            "Model saved to results/training_2025-03-13_10-33/model.pt\n",
            "Weights of the Ensemble models: [0.11926888674497604, 0.3041297495365143, 0.23276644945144653, 0.21991577744483948, 0.12391912937164307]\n",
            "train MODEL: LOSS 0.000003689655 MAE X1:0.0009, Y1:0.0012, Z1:0.0015 R2 X1:0.7522, Y1:0.9611, Z1:0.8312 RMSE X1:0.001293, Y1:0.002277, Z1:0.001983\n",
            "train AUTOENCODER: LOSS 27.233172693560 MAE temp1:2.6531, temp2:6.2194, temp3:2.4013, temp4:2.6675 R2 temp1:0.9791, temp2:0.8856, temp3:0.9799, temp4:0.9651 RMSE temp1:3.371328, temp2:8.073066, temp3:3.080488, temp4:3.217013\n",
            "valid MODEL: LOSS 0.000001285736 MAE X1:0.0007, Y1:0.0007, Z1:0.0010 R2 X1:0.8028, Y1:0.9097, Z1:0.8338 RMSE X1:0.000913, Y1:0.001077, Z1:0.001367\n",
            "valid AUTOENCODER: LOSS 20.833039210393 MAE temp1:1.9593, temp2:6.0726, temp3:2.1511, temp4:2.6675 R2 temp1:0.9814, temp2:0.8718, temp3:0.9796, temp4:0.9651 RMSE temp1:2.638272, temp2:7.635017, temp3:2.584172, temp4:3.217013\n",
            "EPOCH 3/100:\n",
            "Model saved to results/training_2025-03-13_10-33/model.pt\n",
            "Weights of the Ensemble models: [0.11544793099164963, 0.2960629165172577, 0.21093416213989258, 0.2549072802066803, 0.122647725045681]\n",
            "train MODEL: LOSS 0.000002595941 MAE X1:0.0007, Y1:0.0010, Z1:0.0012 R2 X1:0.8515, Y1:0.9687, Z1:0.8850 RMSE X1:0.001001, Y1:0.002042, Z1:0.001636\n",
            "train AUTOENCODER: LOSS 23.472052358812 MAE temp1:2.3286, temp2:5.9142, temp3:2.0702, temp4:2.4079 R2 temp1:0.9838, temp2:0.8978, temp3:0.9849, temp4:0.9710 RMSE temp1:2.972369, temp2:7.637794, temp3:2.663754, temp4:2.942114\n",
            "valid MODEL: LOSS 0.000000841456 MAE X1:0.0005, Y1:0.0006, Z1:0.0008 R2 X1:0.8721, Y1:0.9377, Z1:0.8952 RMSE X1:0.000735, Y1:0.000894, Z1:0.001086\n",
            "valid AUTOENCODER: LOSS 17.521677677448 MAE temp1:1.8721, temp2:5.5904, temp3:1.9973, temp4:2.4079 R2 temp1:0.9842, temp2:0.8920, temp3:0.9826, temp4:0.9710 RMSE temp1:2.438105, temp2:7.016536, temp3:2.388901, temp4:2.942114\n",
            "EPOCH 4/100:\n",
            "Model saved to results/training_2025-03-13_10-33/model.pt\n",
            "Weights of the Ensemble models: [0.11498742550611496, 0.29063206911087036, 0.21102476119995117, 0.26136302947998047, 0.12199266999959946]\n",
            "train MODEL: LOSS 0.000001888160 MAE X1:0.0006, Y1:0.0008, Z1:0.0010 R2 X1:0.8834, Y1:0.9779, Z1:0.9158 RMSE X1:0.000887, Y1:0.001717, Z1:0.001401\n",
            "train AUTOENCODER: LOSS 21.045869058178 MAE temp1:2.1510, temp2:5.6615, temp3:1.9005, temp4:2.2881 R2 temp1:0.9863, temp2:0.9068, temp3:0.9875, temp4:0.9727 RMSE temp1:2.736373, temp2:7.300336, temp3:2.437062, temp4:2.876719\n",
            "valid MODEL: LOSS 0.000000726581 MAE X1:0.0005, Y1:0.0007, Z1:0.0007 R2 X1:0.8898, Y1:0.9313, Z1:0.9264 RMSE X1:0.000682, Y1:0.000939, Z1:0.000910\n",
            "valid AUTOENCODER: LOSS 15.818626000331 MAE temp1:2.1865, temp2:5.0534, temp3:2.1050, temp4:2.2881 R2 temp1:0.9812, temp2:0.9100, temp3:0.9805, temp4:0.9727 RMSE temp1:2.676765, temp2:6.405373, temp3:2.545152, temp4:2.876719\n",
            "EPOCH 5/100:\n",
            "Model saved to results/training_2025-03-13_10-33/model.pt\n",
            "Weights of the Ensemble models: [0.11331065744161606, 0.2840103805065155, 0.22722111642360687, 0.2509239614009857, 0.12453392893075943]\n",
            "train MODEL: LOSS 0.000001701408 MAE X1:0.0006, Y1:0.0008, Z1:0.0010 R2 X1:0.8973, Y1:0.9794, Z1:0.9276 RMSE X1:0.000833, Y1:0.001660, Z1:0.001299\n",
            "train AUTOENCODER: LOSS 19.280948792734 MAE temp1:2.0398, temp2:5.4722, temp3:1.7480, temp4:2.0742 R2 temp1:0.9876, temp2:0.9130, temp3:0.9895, temp4:0.9775 RMSE temp1:2.597013, temp2:7.051573, temp3:2.242985, temp4:2.621000\n",
            "valid MODEL: LOSS 0.000000562803 MAE X1:0.0004, Y1:0.0006, Z1:0.0006 R2 X1:0.9133, Y1:0.9522, Z1:0.9371 RMSE X1:0.000605, Y1:0.000784, Z1:0.000841\n",
            "valid AUTOENCODER: LOSS 13.977862101335 MAE temp1:1.9764, temp2:4.8058, temp3:1.8683, temp4:2.0742 R2 temp1:0.9837, temp2:0.9182, temp3:0.9841, temp4:0.9775 RMSE temp1:2.491744, temp2:6.105011, temp3:2.307100, temp4:2.621000\n",
            "EPOCH 6/100:\n",
            "Model saved to results/training_2025-03-13_10-33/model.pt\n",
            "Weights of the Ensemble models: [0.11024995148181915, 0.2740209698677063, 0.24063752591609955, 0.25482112169265747, 0.12027043849229813]\n",
            "train MODEL: LOSS 0.000001575618 MAE X1:0.0006, Y1:0.0008, Z1:0.0009 R2 X1:0.9066, Y1:0.9805, Z1:0.9357 RMSE X1:0.000794, Y1:0.001615, Z1:0.001224\n",
            "train AUTOENCODER: LOSS 18.041883222518 MAE temp1:1.9704, temp2:5.2963, temp3:1.6668, temp4:1.9094 R2 temp1:0.9884, temp2:0.9184, temp3:0.9905, temp4:0.9797 RMSE temp1:2.516851, temp2:6.828164, temp3:2.144550, temp4:2.488289\n",
            "valid MODEL: LOSS 0.000000527842 MAE X1:0.0004, Y1:0.0006, Z1:0.0006 R2 X1:0.9224, Y1:0.9544, Z1:0.9406 RMSE X1:0.000573, Y1:0.000766, Z1:0.000818\n",
            "valid AUTOENCODER: LOSS 12.956347208757 MAE temp1:1.5793, temp2:4.9225, temp3:1.5215, temp4:1.9094 R2 temp1:0.9893, temp2:0.9175, temp3:0.9888, temp4:0.9797 RMSE temp1:2.014716, temp2:6.124545, temp3:1.930994, temp4:2.488289\n",
            "EPOCH 7/100:\n",
            "Model saved to results/training_2025-03-13_10-33/model.pt\n",
            "Weights of the Ensemble models: [0.11115391552448273, 0.2760376036167145, 0.2266354262828827, 0.2641426622867584, 0.12203040719032288]\n",
            "train MODEL: LOSS 0.000001490934 MAE X1:0.0006, Y1:0.0007, Z1:0.0009 R2 X1:0.9115, Y1:0.9811, Z1:0.9410 RMSE X1:0.000773, Y1:0.001589, Z1:0.001173\n",
            "train AUTOENCODER: LOSS 17.455377255717 MAE temp1:1.9084, temp2:5.1922, temp3:1.6181, temp4:1.9652 R2 temp1:0.9889, temp2:0.9216, temp3:0.9909, temp4:0.9781 RMSE temp1:2.458228, temp2:6.693058, temp3:2.089470, temp4:2.577994\n",
            "valid MODEL: LOSS 0.000000488875 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9246, Y1:0.9596, Z1:0.9441 RMSE X1:0.000564, Y1:0.000720, Z1:0.000793\n",
            "valid AUTOENCODER: LOSS 12.913468801058 MAE temp1:1.5763, temp2:4.8512, temp3:1.5636, temp4:1.9652 R2 temp1:0.9892, temp2:0.9196, temp3:0.9880, temp4:0.9781 RMSE temp1:2.027583, temp2:6.044596, temp3:1.994016, temp4:2.577994\n",
            "EPOCH 8/100:\n",
            "Model saved to results/training_2025-03-13_10-33/model.pt\n",
            "Weights of the Ensemble models: [0.11247386038303375, 0.27603891491889954, 0.23449823260307312, 0.25677579641342163, 0.12021317332983017]\n",
            "train MODEL: LOSS 0.000001405932 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9176, Y1:0.9820, Z1:0.9456 RMSE X1:0.000746, Y1:0.001552, Z1:0.001126\n",
            "train AUTOENCODER: LOSS 16.961926829430 MAE temp1:1.8830, temp2:5.1160, temp3:1.6037, temp4:1.8913 R2 temp1:0.9892, temp2:0.9240, temp3:0.9911, temp4:0.9805 RMSE temp1:2.430286, temp2:6.589696, temp3:2.070029, temp4:2.433403\n",
            "valid MODEL: LOSS 0.000000461548 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9306, Y1:0.9608, Z1:0.9477 RMSE X1:0.000542, Y1:0.000709, Z1:0.000767\n",
            "valid AUTOENCODER: LOSS 11.863182104551 MAE temp1:1.5436, temp2:4.6196, temp3:1.5256, temp4:1.8913 R2 temp1:0.9899, temp2:0.9260, temp3:0.9889, temp4:0.9805 RMSE temp1:1.962698, temp2:5.801284, temp3:1.915697, temp4:2.433403\n",
            "EPOCH 9/100:\n",
            "Weights of the Ensemble models: [0.11042934656143188, 0.27048322558403015, 0.2393745481967926, 0.2598898410797119, 0.11982301622629166]\n",
            "train MODEL: LOSS 0.000001334568 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9213, Y1:0.9829, Z1:0.9480 RMSE X1:0.000729, Y1:0.001509, Z1:0.001101\n",
            "train AUTOENCODER: LOSS 16.437287838228 MAE temp1:1.8435, temp2:5.0227, temp3:1.5664, temp4:1.9695 R2 temp1:0.9896, temp2:0.9265, temp3:0.9915, temp4:0.9788 RMSE temp1:2.379054, temp2:6.478614, temp3:2.023872, temp4:2.540017\n",
            "valid MODEL: LOSS 0.000000489352 MAE X1:0.0004, Y1:0.0006, Z1:0.0006 R2 X1:0.9309, Y1:0.9538, Z1:0.9480 RMSE X1:0.000540, Y1:0.000770, Z1:0.000765\n",
            "valid AUTOENCODER: LOSS 12.176375975976 MAE temp1:1.5079, temp2:4.6917, temp3:1.5251, temp4:1.9695 R2 temp1:0.9906, temp2:0.9237, temp3:0.9889, temp4:0.9788 RMSE temp1:1.896432, temp2:5.886059, temp3:1.918565, temp4:2.540017\n",
            "EPOCH 10/100:\n",
            "Weights of the Ensemble models: [0.10902741551399231, 0.2700762450695038, 0.232848659157753, 0.2677290439605713, 0.12031857669353485]\n",
            "train MODEL: LOSS 0.000001292916 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9231, Y1:0.9834, Z1:0.9498 RMSE X1:0.000720, Y1:0.001488, Z1:0.001081\n",
            "train AUTOENCODER: LOSS 15.975138464282 MAE temp1:1.8335, temp2:4.9360, temp3:1.5569, temp4:2.0592 R2 temp1:0.9898, temp2:0.9289, temp3:0.9916, temp4:0.9766 RMSE temp1:2.364442, temp2:6.375222, temp3:2.010269, temp4:2.662805\n",
            "valid MODEL: LOSS 0.000000478077 MAE X1:0.0004, Y1:0.0006, Z1:0.0006 R2 X1:0.9283, Y1:0.9558, Z1:0.9499 RMSE X1:0.000550, Y1:0.000754, Z1:0.000750\n",
            "valid AUTOENCODER: LOSS 13.426458578843 MAE temp1:1.5567, temp2:4.9593, temp3:1.6605, temp4:2.0592 R2 temp1:0.9895, temp2:0.9168, temp3:0.9867, temp4:0.9766 RMSE temp1:1.990470, temp2:6.148777, temp3:2.092163, temp4:2.662805\n",
            "EPOCH 11/100:\n",
            "Model saved to results/training_2025-03-13_10-33/model.pt\n",
            "Weights of the Ensemble models: [0.11203433573246002, 0.2725692391395569, 0.2387215495109558, 0.25601816177368164, 0.12065670639276505]\n",
            "train MODEL: LOSS 0.000001265998 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9265, Y1:0.9835, Z1:0.9514 RMSE X1:0.000704, Y1:0.001482, Z1:0.001064\n",
            "train AUTOENCODER: LOSS 15.400721596133 MAE temp1:1.8205, temp2:4.8321, temp3:1.5505, temp4:2.0649 R2 temp1:0.9899, temp2:0.9317, temp3:0.9917, temp4:0.9758 RMSE temp1:2.348733, temp2:6.246044, temp3:2.001674, temp4:2.716066\n",
            "valid MODEL: LOSS 0.000000451730 MAE X1:0.0004, Y1:0.0005, Z1:0.0005 R2 X1:0.9363, Y1:0.9589, Z1:0.9502 RMSE X1:0.000519, Y1:0.000726, Z1:0.000748\n",
            "valid AUTOENCODER: LOSS 12.467080409710 MAE temp1:1.5254, temp2:4.6688, temp3:1.5948, temp4:2.0649 R2 temp1:0.9904, temp2:0.9242, temp3:0.9879, temp4:0.9758 RMSE temp1:1.910944, temp2:5.866935, temp3:2.002372, temp4:2.716066\n",
            "EPOCH 12/100:\n",
            "Model saved to results/training_2025-03-13_10-33/model.pt\n",
            "Weights of the Ensemble models: [0.11129319667816162, 0.2751145362854004, 0.23447033762931824, 0.2549876868724823, 0.12413426488637924]\n",
            "train MODEL: LOSS 0.000001268523 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9278, Y1:0.9836, Z1:0.9509 RMSE X1:0.000698, Y1:0.001479, Z1:0.001070\n",
            "train AUTOENCODER: LOSS 14.870802064096 MAE temp1:1.8107, temp2:4.7396, temp3:1.5383, temp4:2.2314 R2 temp1:0.9900, temp2:0.9340, temp3:0.9919, temp4:0.9705 RMSE temp1:2.334124, temp2:6.142352, temp3:1.983007, temp4:2.999367\n",
            "valid MODEL: LOSS 0.000000445812 MAE X1:0.0004, Y1:0.0005, Z1:0.0005 R2 X1:0.9383, Y1:0.9601, Z1:0.9499 RMSE X1:0.000511, Y1:0.000715, Z1:0.000751\n",
            "valid AUTOENCODER: LOSS 13.312618805812 MAE temp1:1.6246, temp2:4.6563, temp3:1.7429, temp4:2.2314 R2 temp1:0.9890, temp2:0.9235, temp3:0.9851, temp4:0.9705 RMSE temp1:2.038716, temp2:5.892874, temp3:2.219111, temp4:2.999367\n",
            "EPOCH 13/100:\n",
            "Weights of the Ensemble models: [0.10784208029508591, 0.2670021057128906, 0.24135029315948486, 0.26221054792404175, 0.12159501761198044]\n",
            "train MODEL: LOSS 0.000001236411 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9305, Y1:0.9840, Z1:0.9517 RMSE X1:0.000685, Y1:0.001462, Z1:0.001061\n",
            "train AUTOENCODER: LOSS 14.077077573346 MAE temp1:1.8011, temp2:4.6167, temp3:1.5172, temp4:2.1867 R2 temp1:0.9901, temp2:0.9371, temp3:0.9921, temp4:0.9725 RMSE temp1:2.325060, temp2:5.995802, temp3:1.953781, temp4:2.899406\n",
            "valid MODEL: LOSS 0.000000466634 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9362, Y1:0.9573, Z1:0.9482 RMSE X1:0.000519, Y1:0.000740, Z1:0.000764\n",
            "valid AUTOENCODER: LOSS 13.361934845264 MAE temp1:1.5332, temp2:4.7276, temp3:1.7064, temp4:2.1867 R2 temp1:0.9902, temp2:0.9201, temp3:0.9860, temp4:0.9725 RMSE temp1:1.924402, temp2:6.025906, temp3:2.153731, temp4:2.899406\n",
            "EPOCH 14/100:\n",
            "Weights of the Ensemble models: [0.11124087125062943, 0.27386224269866943, 0.23352296650409698, 0.2587282657623291, 0.12264566123485565]\n",
            "train MODEL: LOSS 0.000001263464 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9292, Y1:0.9837, Z1:0.9504 RMSE X1:0.000691, Y1:0.001477, Z1:0.001075\n",
            "train AUTOENCODER: LOSS 13.498574472243 MAE temp1:1.8030, temp2:4.5157, temp3:1.5016, temp4:2.2204 R2 temp1:0.9901, temp2:0.9395, temp3:0.9923, temp4:0.9706 RMSE temp1:2.328970, temp2:5.881964, temp3:1.934281, temp4:2.993036\n",
            "valid MODEL: LOSS 0.000000474267 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9391, Y1:0.9589, Z1:0.9433 RMSE X1:0.000507, Y1:0.000727, Z1:0.000799\n",
            "valid AUTOENCODER: LOSS 12.511803076817 MAE temp1:1.5977, temp2:4.3956, temp3:1.7269, temp4:2.2204 R2 temp1:0.9893, temp2:0.9304, temp3:0.9850, temp4:0.9706 RMSE temp1:2.015826, temp2:5.624652, temp3:2.227773, temp4:2.993036\n",
            "EPOCH 15/100:\n",
            "Weights of the Ensemble models: [0.11076299846172333, 0.27451416850090027, 0.23372702300548553, 0.2576185464859009, 0.12337729334831238]\n",
            "train MODEL: LOSS 0.000001233339 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9296, Y1:0.9843, Z1:0.9507 RMSE X1:0.000689, Y1:0.001447, Z1:0.001072\n",
            "train AUTOENCODER: LOSS 12.888676504935 MAE temp1:1.8031, temp2:4.4156, temp3:1.4881, temp4:2.0863 R2 temp1:0.9900, temp2:0.9418, temp3:0.9925, temp4:0.9750 RMSE temp1:2.331474, temp2:5.767329, temp3:1.914300, temp4:2.768343\n",
            "valid MODEL: LOSS 0.000000478164 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9372, Y1:0.9599, Z1:0.9418 RMSE X1:0.000515, Y1:0.000718, Z1:0.000809\n",
            "valid AUTOENCODER: LOSS 11.474646384899 MAE temp1:1.7326, temp2:4.1439, temp3:1.7549, temp4:2.0863 R2 temp1:0.9879, temp2:0.9374, temp3:0.9852, temp4:0.9750 RMSE temp1:2.144532, temp2:5.335448, temp3:2.219160, temp4:2.768343\n",
            "EPOCH 16/100:\n",
            "Weights of the Ensemble models: [0.10847613215446472, 0.27157264947891235, 0.23578859865665436, 0.2611834704875946, 0.12297911196947098]\n",
            "train MODEL: LOSS 0.000001261345 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9285, Y1:0.9840, Z1:0.9486 RMSE X1:0.000695, Y1:0.001460, Z1:0.001095\n",
            "train AUTOENCODER: LOSS 12.384987692679 MAE temp1:1.8128, temp2:4.3402, temp3:1.4834, temp4:2.0511 R2 temp1:0.9899, temp2:0.9435, temp3:0.9926, temp4:0.9752 RMSE temp1:2.343150, temp2:5.681274, temp3:1.905244, temp4:2.753107\n",
            "valid MODEL: LOSS 0.000000496740 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9371, Y1:0.9578, Z1:0.9393 RMSE X1:0.000516, Y1:0.000736, Z1:0.000826\n",
            "valid AUTOENCODER: LOSS 11.863234996796 MAE temp1:1.6589, temp2:4.3164, temp3:1.7031, temp4:2.0511 R2 temp1:0.9889, temp2:0.9323, temp3:0.9861, temp4:0.9752 RMSE temp1:2.050598, temp2:5.548383, temp3:2.149920, temp4:2.753107\n",
            "EPOCH 17/100:\n",
            "Weights of the Ensemble models: [0.10923111438751221, 0.27296191453933716, 0.23213231563568115, 0.2592827081680298, 0.12639187276363373]\n",
            "train MODEL: LOSS 0.000001232907 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9287, Y1:0.9846, Z1:0.9494 RMSE X1:0.000694, Y1:0.001432, Z1:0.001086\n",
            "train AUTOENCODER: LOSS 11.967131537776 MAE temp1:1.8236, temp2:4.2707, temp3:1.4658, temp4:2.0572 R2 temp1:0.9898, temp2:0.9453, temp3:0.9928, temp4:0.9761 RMSE temp1:2.355838, temp2:5.593175, temp3:1.881490, temp4:2.701655\n",
            "valid MODEL: LOSS 0.000000490635 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9387, Y1:0.9589, Z1:0.9393 RMSE X1:0.000509, Y1:0.000727, Z1:0.000827\n",
            "valid AUTOENCODER: LOSS 11.564630618462 MAE temp1:1.6197, temp2:4.2530, temp3:1.6447, temp4:2.0572 R2 temp1:0.9892, temp2:0.9339, temp3:0.9865, temp4:0.9761 RMSE temp1:2.025935, temp2:5.486784, temp3:2.114958, temp4:2.701655\n",
            "EPOCH 18/100:\n",
            "Weights of the Ensemble models: [0.10962260514497757, 0.2736383378505707, 0.23013979196548462, 0.2623864710330963, 0.1242128312587738]\n",
            "train MODEL: LOSS 0.000001238857 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9282, Y1:0.9844, Z1:0.9496 RMSE X1:0.000696, Y1:0.001444, Z1:0.001083\n",
            "train AUTOENCODER: LOSS 11.692699678483 MAE temp1:1.8320, temp2:4.2135, temp3:1.4605, temp4:1.9746 R2 temp1:0.9897, temp2:0.9467, temp3:0.9928, temp4:0.9766 RMSE temp1:2.372639, temp2:5.519451, temp3:1.875305, temp4:2.674904\n",
            "valid MODEL: LOSS 0.000000498328 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9377, Y1:0.9589, Z1:0.9374 RMSE X1:0.000513, Y1:0.000727, Z1:0.000839\n",
            "valid AUTOENCODER: LOSS 11.094940222227 MAE temp1:1.7567, temp2:4.0810, temp3:1.6929, temp4:1.9746 R2 temp1:0.9877, temp2:0.9394, temp3:0.9859, temp4:0.9766 RMSE temp1:2.163940, temp2:5.253377, temp3:2.164756, temp4:2.674904\n",
            "EPOCH 19/100:\n",
            "Weights of the Ensemble models: [0.1086314469575882, 0.2722126543521881, 0.22860103845596313, 0.26499566435813904, 0.12555918097496033]\n",
            "train MODEL: LOSS 0.000001247862 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9257, Y1:0.9845, Z1:0.9485 RMSE X1:0.000708, Y1:0.001438, Z1:0.001095\n",
            "train AUTOENCODER: LOSS 11.522157007648 MAE temp1:1.8350, temp2:4.1795, temp3:1.4602, temp4:1.9744 R2 temp1:0.9897, temp2:0.9475, temp3:0.9928, temp4:0.9787 RMSE temp1:2.375008, temp2:5.478421, temp3:1.872718, temp4:2.555118\n",
            "valid MODEL: LOSS 0.000000504149 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9359, Y1:0.9583, Z1:0.9373 RMSE X1:0.000521, Y1:0.000731, Z1:0.000840\n",
            "valid AUTOENCODER: LOSS 10.706078015841 MAE temp1:1.7047, temp2:4.0687, temp3:1.6120, temp4:1.9744 R2 temp1:0.9885, temp2:0.9395, temp3:0.9876, temp4:0.9787 RMSE temp1:2.095383, temp2:5.250984, temp3:2.036113, temp4:2.555118\n",
            "EPOCH 20/100:\n",
            "Weights of the Ensemble models: [0.10988319665193558, 0.2724139988422394, 0.23537136614322662, 0.257750928401947, 0.12458056211471558]\n",
            "train MODEL: LOSS 0.000001233425 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9278, Y1:0.9845, Z1:0.9497 RMSE X1:0.000698, Y1:0.001436, Z1:0.001082\n",
            "train AUTOENCODER: LOSS 11.299850648449 MAE temp1:1.8414, temp2:4.1360, temp3:1.4491, temp4:1.9257 R2 temp1:0.9896, temp2:0.9485, temp3:0.9930, temp4:0.9785 RMSE temp1:2.380020, temp2:5.422445, temp3:1.857940, temp4:2.567574\n",
            "valid MODEL: LOSS 0.000000491778 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9356, Y1:0.9596, Z1:0.9392 RMSE X1:0.000522, Y1:0.000720, Z1:0.000827\n",
            "valid AUTOENCODER: LOSS 10.506122625791 MAE temp1:1.7398, temp2:3.9901, temp3:1.6426, temp4:1.9257 R2 temp1:0.9879, temp2:0.9424, temp3:0.9868, temp4:0.9785 RMSE temp1:2.142232, temp2:5.123364, temp3:2.100463, temp4:2.567574\n",
            "EPOCH 21/100:\n",
            "Weights of the Ensemble models: [0.10901469737291336, 0.27342838048934937, 0.2359844148159027, 0.25722840428352356, 0.12434409558773041]\n",
            "train MODEL: LOSS 0.000001225468 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9262, Y1:0.9848, Z1:0.9499 RMSE X1:0.000706, Y1:0.001426, Z1:0.001081\n",
            "train AUTOENCODER: LOSS 11.116271618874 MAE temp1:1.8386, temp2:4.1015, temp3:1.4445, temp4:1.8802 R2 temp1:0.9896, temp2:0.9494, temp3:0.9930, temp4:0.9805 RMSE temp1:2.382172, temp2:5.378288, temp3:1.850485, temp4:2.443103\n",
            "valid MODEL: LOSS 0.000000506179 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9344, Y1:0.9573, Z1:0.9384 RMSE X1:0.000527, Y1:0.000740, Z1:0.000832\n",
            "valid AUTOENCODER: LOSS 10.230841306540 MAE temp1:1.6500, temp2:4.0160, temp3:1.5817, temp4:1.8802 R2 temp1:0.9891, temp2:0.9417, temp3:0.9879, temp4:0.9805 RMSE temp1:2.039488, temp2:5.154030, temp3:2.008734, temp4:2.443103\n",
            "EPOCH 22/100:\n",
            "Weights of the Ensemble models: [0.10807915031909943, 0.2710917890071869, 0.2367399036884308, 0.2581416964530945, 0.1259474903345108]\n",
            "train MODEL: LOSS 0.000001243085 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9256, Y1:0.9845, Z1:0.9495 RMSE X1:0.000709, Y1:0.001439, Z1:0.001085\n",
            "train AUTOENCODER: LOSS 11.012113786513 MAE temp1:1.8434, temp2:4.0737, temp3:1.4426, temp4:1.8354 R2 temp1:0.9896, temp2:0.9500, temp3:0.9931, temp4:0.9809 RMSE temp1:2.387720, temp2:5.342697, temp3:1.846762, temp4:2.417623\n",
            "valid MODEL: LOSS 0.000000486060 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9352, Y1:0.9604, Z1:0.9398 RMSE X1:0.000523, Y1:0.000713, Z1:0.000823\n",
            "valid AUTOENCODER: LOSS 10.330547332764 MAE temp1:1.6770, temp2:4.0468, temp3:1.5869, temp4:1.8354 R2 temp1:0.9888, temp2:0.9408, temp3:0.9878, temp4:0.9809 RMSE temp1:2.067792, temp2:5.195033, temp3:2.016194, temp4:2.417623\n",
            "EPOCH 23/100:\n",
            "Weights of the Ensemble models: [0.10811586678028107, 0.267045259475708, 0.24061812460422516, 0.2584119737148285, 0.12580879032611847]\n",
            "train MODEL: LOSS 0.000001221820 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9279, Y1:0.9848, Z1:0.9498 RMSE X1:0.000697, Y1:0.001423, Z1:0.001081\n",
            "train AUTOENCODER: LOSS 10.881849135122 MAE temp1:1.8521, temp2:4.0536, temp3:1.4455, temp4:1.8139 R2 temp1:0.9895, temp2:0.9505, temp3:0.9930, temp4:0.9818 RMSE temp1:2.398616, temp2:5.315854, temp3:1.853837, temp4:2.365050\n",
            "valid MODEL: LOSS 0.000000489764 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9332, Y1:0.9593, Z1:0.9407 RMSE X1:0.000531, Y1:0.000723, Z1:0.000817\n",
            "valid AUTOENCODER: LOSS 9.935045682467 MAE temp1:1.7036, temp2:3.9399, temp3:1.5873, temp4:1.8139 R2 temp1:0.9884, temp2:0.9440, temp3:0.9878, temp4:0.9818 RMSE temp1:2.101023, temp2:5.051941, temp3:2.018166, temp4:2.365050\n",
            "EPOCH 24/100:\n",
            "Weights of the Ensemble models: [0.10938052833080292, 0.27008041739463806, 0.2323599010705948, 0.26273900270462036, 0.12544013559818268]\n",
            "train MODEL: LOSS 0.000001217797 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9263, Y1:0.9848, Z1:0.9511 RMSE X1:0.000705, Y1:0.001425, Z1:0.001067\n",
            "train AUTOENCODER: LOSS 10.793674622813 MAE temp1:1.8419, temp2:4.0338, temp3:1.4369, temp4:1.8404 R2 temp1:0.9895, temp2:0.9511, temp3:0.9931, temp4:0.9817 RMSE temp1:2.387914, temp2:5.286211, temp3:1.841450, temp4:2.370783\n",
            "valid MODEL: LOSS 0.000000488181 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9333, Y1:0.9597, Z1:0.9407 RMSE X1:0.000531, Y1:0.000719, Z1:0.000817\n",
            "valid AUTOENCODER: LOSS 9.890644990481 MAE temp1:1.7159, temp2:3.9164, temp3:1.5763, temp4:1.8404 R2 temp1:0.9883, temp2:0.9445, temp3:0.9880, temp4:0.9817 RMSE temp1:2.114001, temp2:5.029193, temp3:2.005702, temp4:2.370783\n",
            "EPOCH 25/100:\n",
            "Weights of the Ensemble models: [0.11076577007770538, 0.2739214599132538, 0.22382785379886627, 0.2639656960964203, 0.12751923501491547]\n",
            "train MODEL: LOSS 0.000001211476 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9287, Y1:0.9848, Z1:0.9507 RMSE X1:0.000694, Y1:0.001425, Z1:0.001072\n",
            "train AUTOENCODER: LOSS 10.721219816516 MAE temp1:1.8408, temp2:4.0223, temp3:1.4332, temp4:1.7895 R2 temp1:0.9896, temp2:0.9514, temp3:0.9931, temp4:0.9824 RMSE temp1:2.387598, temp2:5.271523, temp3:1.837116, temp4:2.325146\n",
            "valid MODEL: LOSS 0.000000487203 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9329, Y1:0.9594, Z1:0.9417 RMSE X1:0.000532, Y1:0.000722, Z1:0.000810\n",
            "valid AUTOENCODER: LOSS 9.767765925481 MAE temp1:1.7286, temp2:3.8861, temp3:1.5845, temp4:1.7895 R2 temp1:0.9880, temp2:0.9454, temp3:0.9878, temp4:0.9824 RMSE temp1:2.131972, temp2:4.988024, temp3:2.017110, temp4:2.325146\n",
            "EPOCH 26/100:\n",
            "Weights of the Ensemble models: [0.11045646667480469, 0.27328458428382874, 0.23625658452510834, 0.25374147295951843, 0.1262609362602234]\n",
            "train MODEL: LOSS 0.000001231651 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9272, Y1:0.9851, Z1:0.9504 RMSE X1:0.000701, Y1:0.001411, Z1:0.001074\n",
            "train AUTOENCODER: LOSS 10.619303226471 MAE temp1:1.8452, temp2:4.0038, temp3:1.4326, temp4:1.8141 R2 temp1:0.9895, temp2:0.9518, temp3:0.9931, temp4:0.9817 RMSE temp1:2.392039, temp2:5.249900, temp3:1.835576, temp4:2.371798\n",
            "valid MODEL: LOSS 0.000000487096 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9329, Y1:0.9583, Z1:0.9430 RMSE X1:0.000533, Y1:0.000732, Z1:0.000801\n",
            "valid AUTOENCODER: LOSS 9.826287636390 MAE temp1:1.7150, temp2:3.8930, temp3:1.5774, temp4:1.8141 R2 temp1:0.9882, temp2:0.9453, temp3:0.9877, temp4:0.9817 RMSE temp1:2.121631, temp2:4.990741, temp3:2.024544, temp4:2.371798\n",
            "EPOCH 27/100:\n",
            "Weights of the Ensemble models: [0.10829266905784607, 0.27017244696617126, 0.23264046013355255, 0.2631397545337677, 0.125754714012146]\n",
            "train MODEL: LOSS 0.000001207141 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9268, Y1:0.9850, Z1:0.9504 RMSE X1:0.000703, Y1:0.001415, Z1:0.001075\n",
            "train AUTOENCODER: LOSS 10.581931852525 MAE temp1:1.8454, temp2:3.9937, temp3:1.4302, temp4:1.7770 R2 temp1:0.9895, temp2:0.9520, temp3:0.9931, temp4:0.9825 RMSE temp1:2.393324, temp2:5.234521, temp3:1.834914, temp4:2.317463\n",
            "valid MODEL: LOSS 0.000000484184 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9321, Y1:0.9594, Z1:0.9425 RMSE X1:0.000536, Y1:0.000722, Z1:0.000804\n",
            "valid AUTOENCODER: LOSS 9.687764314505 MAE temp1:1.7318, temp2:3.8713, temp3:1.5823, temp4:1.7770 R2 temp1:0.9880, temp2:0.9459, temp3:0.9879, temp4:0.9825 RMSE temp1:2.133136, temp2:4.962982, temp3:2.013257, temp4:2.317463\n",
            "EPOCH 28/100:\n",
            "Weights of the Ensemble models: [0.10940287262201309, 0.2711494266986847, 0.24341106414794922, 0.25225353240966797, 0.12378309667110443]\n",
            "train MODEL: LOSS 0.000001204759 MAE X1:0.0005, Y1:0.0007, Z1:0.0008 R2 X1:0.9270, Y1:0.9849, Z1:0.9515 RMSE X1:0.000702, Y1:0.001420, Z1:0.001063\n",
            "train AUTOENCODER: LOSS 10.545580325588 MAE temp1:1.8406, temp2:3.9852, temp3:1.4224, temp4:1.8187 R2 temp1:0.9896, temp2:0.9522, temp3:0.9932, temp4:0.9822 RMSE temp1:2.386015, temp2:5.225865, temp3:1.825451, temp4:2.338362\n",
            "valid MODEL: LOSS 0.000000478972 MAE X1:0.0004, Y1:0.0005, Z1:0.0006 R2 X1:0.9326, Y1:0.9592, Z1:0.9443 RMSE X1:0.000534, Y1:0.000724, Z1:0.000792\n",
            "valid AUTOENCODER: LOSS 9.715440933521 MAE temp1:1.6903, temp2:3.8965, temp3:1.5575, temp4:1.8187 R2 temp1:0.9885, temp2:0.9452, temp3:0.9882, temp4:0.9822 RMSE temp1:2.088256, temp2:4.994906, temp3:1.983375, temp4:2.338362\n",
            "EPOCH 29/100:\n"
          ]
        }
      ],
      "source": [
        "if hyper_parameters['pretrain']:\n",
        "    model.load('./results/training_2025-03-08_21-39/autoencoder.pt',autoencoder=hyper_parameters['pretrain'])\n",
        "\n",
        "train(\n",
        "    num_epochs=hyper_parameters['num_epochs'],\n",
        "    loss_fn=loss_fn,\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    training_dataloader=loaderTrain,\n",
        "    validation_dataloader=loaderVal,\n",
        "    hyperparams=hyper_parameters,\n",
        "    model_dict = ensemble_model,\n",
        "    autoencoder_dict=autoencoder_dict,\n",
        "    complete=True,\n",
        "    autoencoder_train=hyper_parameters['autoencoder_train'],\n",
        "    autoencoder=hyper_parameters['extractor_type'] == 'lstm_autoencoder' or hyper_parameters['extractor_type'] == 'conv'\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
